const video = document.getElementById("video");
const overlay = document.getElementById("overlay");
const ctx = overlay.getContext("2d");
const status = document.getElementById("status");

async function start() {
  // face-api.js ã®ãƒ¢ãƒ‡ãƒ«ã‚’CDNã‹ã‚‰èª­ã¿è¾¼ã¿
  await faceapi.nets.tinyFaceDetector.loadFromUri(
    "https://justadudewhohacks.github.io/face-api.js/models"
  );
  await faceapi.nets.faceExpressionNet.loadFromUri(
    "https://justadudewhohacks.github.io/face-api.js/models"
  );

  // ã‚«ãƒ¡ãƒ©ã‚’èµ·å‹•
  navigator.mediaDevices.getUserMedia({ video: true }).then((stream) => {
    video.srcObject = stream;
  });
}

video.addEventListener("play", () => {
  // å®Ÿéš›ã®ã‚«ãƒ¡ãƒ©æ˜ åƒã‚µã‚¤ã‚ºã«åˆã‚ã›ã‚‹
  overlay.width = video.videoWidth;
  overlay.height = video.videoHeight;

  setInterval(async () => {
    const displaySize = { width: video.videoWidth, height: video.videoHeight };
    faceapi.matchDimensions(overlay, displaySize);

    // é¡”æ¤œå‡º
    const detections = await faceapi
      .detectAllFaces(video, new faceapi.TinyFaceDetectorOptions())
      .withFaceExpressions();

    const resized = faceapi.resizeResults(detections, displaySize);

    ctx.clearRect(0, 0, overlay.width, overlay.height);

    ctx.save();
    ctx.scale(-1, 1);
    ctx.translate(-overlay.width, 0);

    faceapi.draw.drawDetections(overlay, resized);
    faceapi.draw.drawFaceExpressions(overlay, resized);

    ctx.restore();

    if (resized.length > 0 && resized[0].expressions) {
      if (resized[0].expressions.happy > 0.7) {
        status.innerText = "ã„ã„ç¬‘é¡”ï¼ã„ã£ã¦ã‚‰ã£ã—ã‚ƒã„ğŸ˜Š";
      } else {
        status.innerText = "ç¬‘é¡”ãŒè¶³ã‚Šãªã„ğŸ˜¢";
      }
    } else {
      status.innerText = "ã‚«ãƒ¡ãƒ©ã‚’èµ·å‹•ä¸­...";
    }
  }, 200);
});
